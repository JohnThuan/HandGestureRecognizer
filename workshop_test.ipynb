{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Gesture Recognition using MediaPipe\n",
    "## Step 1: Setting up video capture\n",
    "We’ll import OpenCV to set up our video capture, where we’ll see our live feed in an application window.\n",
    "\n",
    "**Note:**\n",
    "Video capture frames are in BGR, not RGB, so we’ll have to do convert back and forth before/after processing\n"
   ],
   "id": "ac31ae36c2d649be"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-08T02:55:18.983275Z",
     "start_time": "2025-11-08T02:55:14.533680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "\n",
    "vid_cap = cv2.VideoCapture(0)\n",
    "if not vid_cap.isOpened():\n",
    "\tprint(\"Error in opening video capture\")\n",
    "\texit(1)\n",
    "\n",
    "while True:\n",
    "\tsuccess, image = vid_cap.read()\n",
    "\tif not success:\n",
    "\t\tprint(\"Error in reading in frames\")\n",
    "\t\tbreak\n",
    "\n",
    "\tcv2.imshow(\"Return feed\", image)\n",
    "\tif cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "\t\tbreak\n",
    "\n",
    "vid_cap.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 2: Adding MediaPipe\n",
    "### Options\n",
    "- Parameter objects fed to MediaPipe Tasks/Solutions\n",
    "- You can see we have `BaseOptions` and `GestureRecognitionOptions`, meant for generic MediaPipe models and model specific tuning parameters respectively\n",
    "- Converting BGR to RGB and back for display\n",
    "\n"
   ],
   "id": "4bf5b1822886b655"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T02:55:20.746700Z",
     "start_time": "2025-11-08T02:55:19.000490Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "from mediapipe.tasks.python.vision import GestureRecognizer\n",
    "from mediapipe.tasks.python.vision.core.vision_task_running_mode import VisionTaskRunningMode\n",
    "from mediapipe.tasks.python import BaseOptions\n",
    "from mediapipe.tasks.python.vision.gesture_recognizer import GestureRecognizerOptions\n",
    "import mediapipe as mp\n",
    "\n",
    "def print_result(result, image, timestamp):\n",
    "\tpass\n",
    "\n",
    "\n",
    "options = GestureRecognizerOptions(\n",
    "\tbase_options=BaseOptions(model_asset_path='assets/gesture_recognizer.task'),\n",
    "\trunning_mode=VisionTaskRunningMode.LIVE_STREAM,\n",
    "\tnum_hands=2,\n",
    "\tmin_tracking_confidence=0.4,\n",
    "\tmin_hand_detection_confidence=0.7,\n",
    "\tmin_hand_presence_confidence=0.6,\n",
    "\tresult_callback=print_result)\n",
    "\n",
    "vid_cap = cv2.VideoCapture(0)\n",
    "if not vid_cap.isOpened():\n",
    "\tprint(\"Error in opening video capture\")\n",
    "\texit(1)\n",
    "\n",
    "with GestureRecognizer.create_from_options(options) as recognizer:\n",
    "\twhile True:\n",
    "\t\tsuccess, image = vid_cap.read()\n",
    "\t\tif not success:\n",
    "\t\t\tprint(\"Error in reading in live frames\")\n",
    "\t\t\texit(1)\n",
    "\t\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\t\timage.flags.writeable = False\n",
    "\n",
    "\t\timg_arr = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)\n",
    "\t\trecognizer.recognize_async(img_arr, int(time.time() * 1000))\n",
    "\n",
    "\t\timage.flags.writeable = True\n",
    "\t\timage = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "\t\tcv2.imshow('Return Feed', image)\n",
    "\t\tif cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "\t\t\tbreak\n",
    "\n",
    "vid_cap.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "id": "c30d3aa1842dfde",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 3: Drawing on landmarks\n",
    "We've been able to do the recognition, but there's no visual feedback to verify, analyze, or implement this. Let's change that with landmark drawing"
   ],
   "id": "699d2bccbc73ea18"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T02:55:20.763394Z",
     "start_time": "2025-11-08T02:55:20.757580Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from mediapipe.python.solutions.drawing_utils import DrawingSpec\n",
    "from mediapipe.python import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "\n",
    "\n",
    "def draw_image_landmarks(og_img, result):\n",
    "\t# we need the hand landmarks, the handedness (for left or right detection), and the gestures properties from our result object\n",
    "\t# Additionally, we need to make a copy of our original frame, we'll annotate on this and display IT instead of the original\n",
    "\thand_landmarks_list = result.hand_landmarks\n",
    "\thandedness_list = result.handedness\n",
    "\tgesture = result.gestures\n",
    "\tannotated_image = np.copy(og_img)\n",
    "\n",
    "\t# hand_landmarks_list is a nested containing landmarks for each hand detected in the frame (up to max numHands)\n",
    "\tfor idx in range(len(hand_landmarks_list)):\n",
    "\t\thand_landmarks = hand_landmarks_list[idx]\n",
    "\t\thandedness = handedness_list[idx]\n",
    "\n",
    "\t\t# Draw the hand landmarks.\n",
    "\t\t# Normalize the coordinates of the landmark (x,y,z are all 0 to 1)\n",
    "\t\thand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "\t\thand_landmarks_proto.landmark.extend([\n",
    "\t\t\tlandmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks\n",
    "\t\t])\n",
    "\n",
    "\t\t# Use drawing solution to draw and connect landmarks\n",
    "\t\tsolutions.drawing_utils.draw_landmarks(\n",
    "\t\t\tannotated_image,\n",
    "\t\t\thand_landmarks_proto,\n",
    "\t\t\tsolutions.hands.HAND_CONNECTIONS,\n",
    "\t\t\tDrawingSpec(color=(0, 255, 240)),\n",
    "\t\t\tDrawingSpec(color=(0, 255, 240)))\n",
    "\n",
    "\t\t# Get the top left corner of the detected hand's bounding box.\n",
    "\t\theight, width, _ = annotated_image.shape\n",
    "\t\tx_coordinates = [landmark.x for landmark in hand_landmarks]\n",
    "\t\ty_coordinates = [landmark.y for landmark in hand_landmarks]\n",
    "\t\ttext_x = int(min(x_coordinates) * width)\n",
    "\t\ttext_y = int(min(y_coordinates) * height) - 10\n",
    "\n",
    "\t\t# Draw handedness (left or right hand) on the image.\n",
    "\t\tsign = gesture[idx][0] if gesture[idx] else ''\n",
    "\t\tcv2.putText(annotated_image, f\"{handedness[0].category_name}:{sign.category_name if sign else ''}\",\n",
    "\t\t\t\t\t(text_x, text_y), cv2.FONT_HERSHEY_DUPLEX,\n",
    "\t\t\t\t\t1, (0, 0, 0), 3, cv2.LINE_AA)\n",
    "\n",
    "\treturn annotated_image"
   ],
   "id": "8e31f80a7b0b682e",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's also make changes to our existing code to align with this approach",
   "id": "aa205f0d9bf2f1bd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T03:23:12.976652Z",
     "start_time": "2025-11-08T03:23:03.910985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "RESULT = None\n",
    "\n",
    "\n",
    "def print_result(result, image, timestamp):\n",
    "\tglobal RESULT\n",
    "\tRESULT = result\n",
    "\n",
    "\n",
    "options = GestureRecognizerOptions(\n",
    "\tbase_options=BaseOptions(model_asset_path='assets/gesture_recognizer.task'),\n",
    "\trunning_mode=VisionTaskRunningMode.LIVE_STREAM,\n",
    "\tnum_hands=2,\n",
    "\tmin_tracking_confidence=0.4,\n",
    "\tmin_hand_detection_confidence=0.7,\n",
    "\tmin_hand_presence_confidence=0.6,\n",
    "\tresult_callback=print_result)\n",
    "\n",
    "vid_cap = cv2.VideoCapture(0)\n",
    "if not vid_cap.isOpened():\n",
    "\tprint(\"Error in opening video capture\")\n",
    "\texit(1)\n",
    "\n",
    "with GestureRecognizer.create_from_options(options) as recognizer:\n",
    "\twhile True:\n",
    "\t\tsuccess, image = vid_cap.read()\n",
    "\t\tif not success:\n",
    "\t\t\tprint(\"Error in reading in live frames\")\n",
    "\t\t\tbreak\n",
    "\n",
    "\t\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\t\timage.flags.writeable = False\n",
    "\n",
    "\t\ttimestamp = int(round(time.time() * 1000))\n",
    "\t\tmp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)\n",
    "\n",
    "\t\trecognizer.recognize_async(mp_image, timestamp)\n",
    "\n",
    "\t\timage.flags.writeable = True\n",
    "\t\timage = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "\t\tif RESULT:\n",
    "\t\t\timage = draw_image_landmarks(image, RESULT)\n",
    "\n",
    "\t\tnew_win = cv2.resize(image, (700, 500))\n",
    "\t\tcv2.imshow('Return Feed', new_win)\n",
    "\t\tif cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "\t\t\tbreak\n",
    "\n",
    "vid_cap.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "id": "f74a714efa3904ed",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ALL DONE",
   "id": "10da98486bdded7f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
